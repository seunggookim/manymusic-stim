---
title: Home
layout: home
---

# ManyMusic-Stim🎵
<!-- # ManyMusic-Stim: An Open-access Music Audio Dataset for Human Experiments on Musical Emotions -->

We[^1] presents the `ManyMusic-Stim` dataset, which is an open-access music audio dataset designed for human experiments on musical emotions.

## 🤔Another music dataset, why?
Psychological and neuroscientific research on music-evoked emotions has been constrained by limitations in stimulus selection. Common issues include artificially manipulated stimuli, copyright restrictions preventing data sharing, narrow genre sampling, and experimenter bias. The ManyMusic-Stim dataset addresses these challenges by offering a large-scale, open-access collection of music audio files curated to ensure diverse representation across genres and styles.

## 🔬How was it validated?
We conducted a series of experiments comparing the ManyMusic-Stim dataset to commercial music. Our results show that [carefully curated](/plots_music) subsets of the dataset are comparable to well-matched commercial tracks across various subjective ratings, including liking and feeling moved. These [findings](/plots_bhv) provide empirical support for the use of ManyMusic-Stim as a valid alternative to commercial music in affective research.

See our [paper] for more details.


## 😀How to use it?
The ManyMusic-Stim dataset is available for download from [Zenodo].
The dataset includes: 

- audio files in MP3 format, 
- metadata in CSV format, and 
- subjective ratings of musical emotions in CSV format.

The audio files are encoded at 320 kbps, and the metadata includes information such as the title, artist, genre, and duration of each track.
The subjective ratings of musical emotions include ratings for valence, arousal,
and dominance, as well as ratings for specific emotions such as happiness, sadness, and anger.

## 📚How to cite?
If you use the ManyMusic-Stim dataset in your research, please cite the following paper:

[PAPER-INFO]
[BIBTEX-INFO]
[RIS-INFO]

## 🙏Acknowledgments
This work was supported by the Max Planck Society and Johanna Quandt Young Academy at Goethe University Frankfurt. 
Part of the data was presented at the [ICMPC] conference in São Paulo, Braizil, 2025.
We thank all participants for their contributions to this dataset.

## 👩‍⚖️License
The ManyMusic-Stim dataset is licensed under ....


<!-- (CC BY-NC-SA 4.0 International License)[https://creativecommons.org/licenses/by-nc-sa/4.0/]. This means you are free to share and adapt the dataset for non-commercial purposes, as long as you give appropriate credit, provide a link to the license, and indicate if changes were made. You may not use the material for commercial purposes. -->

----

[^1]: The work is a collaboration between the [Max Planck Institute for Empirical Aesthetics](https://www.aesthetics.mpg.de/en.html) and [Pompeu Fabra University](https://www.upf.edu/web/mtg). Main contributors are [Seung-Goo Kim](https://github.com/seunggookim/), [Pablo Alonso](https://github.com/palonso), and [Dmitry Bogdanov](https://github.com/dbogdanov).


<!-- The dataset is part of the [ManyMusic project](https://www.aesthetics.mpg.de/en/research/projects/manymusic.html). -->

<!--
[Just the Docs]: https://just-the-docs.github.io/just-the-docs/
[GitHub Pages]: https://docs.github.com/en/pages
[README]: https://github.com/just-the-docs/just-the-docs-template/blob/main/README.md
[Jekyll]: https://jekyllrb.com
[GitHub Pages / Actions workflow]: https://github.blog/changelog/2022-07-27-github-pages-custom-github-actions-workflows-beta/
[use this template]: https://github.com/just-the-docs/just-the-docs-template/generate -->
[Zenodo]: https://zenodo.org/record/1234567
[paper]: paper-link
[ICMPC]: https://www.icmpc.org/
